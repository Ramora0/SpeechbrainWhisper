%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Acoustically Inspired Dynamic Compression in Speech Encoding (ICML 2025)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lee Davis}{equal,yyy}
\icmlauthor{Shinji Watanabe}{equal,yyy,comp}
\icmlauthor{Edoardo Ponti}{comp}
\icmlauthor{Elizabeth Salesky}{sch}
\icmlauthor{Sachin Kumar}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Transformer-based ASR models are bottlenecked by the quadratic cost of self-attention, motivating aggressive downsampling that is typically fixed (e.g., strided convolutions) and therefore unable to decide *where* to compress. We introduce LAPSE (Learned Attention Pooling for Speech Encoding), a learned downsampling module that jointly predicts segment boundaries and pools variable-length spans into single vectors. On LibriSpeech 960h with a 120M-parameter Conformer trained from scratch, LAPSE achieves 10.4× compression when targeting phoneme-level segmentation and 28× compression at the syllable level, while maintaining or improving recognition accuracy relative to fixed convolutional downsampling and other baselines (WER: [TBD], ΔWER vs conv: [TBD]).

\end{abstract}

\section{Introduction}
\label{submission}

Submission to ICML 2025 will be entirely electronic, via a web site
(not email). Information about the submission process and \LaTeX\ templates
are available on the conference web site at:
\begin{center}
\textbf{\texttt{http://icml.cc/}}
\end{center}

The guidelines below will be enforced for initial submissions and
camera-ready copies. Here is a brief summary:
\begin{itemize}
\item Submissions must be in PDF\@. 
\item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
\item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
\item \textbf{Do not include author information or acknowledgements} in your
    initial submission.
\item Your paper should be in \textbf{10 point Times font}.
\item Make sure your PDF file only uses Type-1 fonts.
\item Place figure captions \emph{under} the figure (and omit titles from inside
    the graphic file itself). Place table captions \emph{over} the table.
\item References must include page numbers whenever possible and be as complete
    as possible. Place multiple citations in chronological order.
\item Do not alter the style template; in particular, do not compress the paper
    format by reducing the vertical spaces.
\item Keep your abstract brief and self-contained, one paragraph and roughly
    4--6 sentences. Gross violations will require correction at the
    camera-ready phase. The title should have content words capitalized.
\end{itemize}

\subsection{Submitting Papers}

\textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
author information may appear on the title page or in the paper
itself. \cref{author info} gives further details.

\medskip

Authors must provide their manuscripts in \textbf{PDF} format.
Furthermore, please make sure that files contain only embedded Type-1 fonts
(e.g.,~using the program \texttt{pdffonts} in linux or using
File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
might come from graphics files imported into the document.

Authors using \textbf{Word} must convert their document to PDF\@. Most
of the latest versions of Word have the facility to do this
automatically. Submissions will not be accepted in Word format or any
format other than PDF\@. Really. We're not joking. Don't send Word.

Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
Those using \texttt{latex} and \texttt{dvips} may need the following
two commands:

{\footnotesize
\begin{verbatim}
dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
ps2pdf paper.ps
\end{verbatim}}
It is a zero following the ``-G'', which tells dvips to use
the config.pdf file. Newer \TeX\ distributions don't always need this
option.

Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
results. This program avoids the Type-3 font problem, and supports more
advanced features in the \texttt{microtype} package.

\textbf{Graphics files} should be a reasonable size, and included from
an appropriate format. Use vector formats (.eps/.pdf) for plots,
lossless bitmap formats (.png) for raster graphics with sharp lines, and
jpeg for photo-like images.

The style file uses the \texttt{hyperref} package to make clickable
links in documents. If this causes problems for you, add
\texttt{nohyperref} as one of the options to the \texttt{icml2025}
usepackage statement.


\subsection{Submitting Final Camera-Ready Copy}

The final versions of papers accepted for publication should follow the
same format and naming convention as initial submissions, except that
author information (names and affiliations) should be given. See
\cref{final author} for formatting instructions.

The footnote, ``Preliminary work. Under review by the International
Conference on Machine Learning (ICML). Do not distribute.'' must be
modified to ``\textit{Proceedings of the
$\mathit{42}^{nd}$ International Conference on Machine Learning},
Vancouver, Canada, PMLR 267, 2025.
Copyright 2025 by the author(s).''

For those using the \textbf{\LaTeX} style file, this change (and others) is
handled automatically by simply changing
$\mathtt{\backslash usepackage\{icml2025\}}$ to
$$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
Authors using \textbf{Word} must edit the
footnote on the first page of the document themselves.

Camera-ready copies should have the title of the paper as running head
on each page except the first one. The running title consists of a
single line centered above a horizontal rule which is $1$~point thick.
The running head should be centered, bold and in $9$~point type. The
rule should be $10$~points above the main text. For those using the
\textbf{\LaTeX} style file, the original title is automatically set as running
head using the \texttt{fancyhdr} package which is included in the ICML
2025 style file package. In case that the original title exceeds the
size restrictions, a shorter form can be supplied by using

\verb|\icmltitlerunning{...}|

just before $\mathtt{\backslash begin\{document\}}$.
Authors using \textbf{Word} must edit the header of the document themselves.

\section{Architecture}

All submissions must follow the specified format.

\subsection{Overview and placement}
\label{sec:lapse_overview}
Transformer-based ASR models often rely on fixed-width strided subsampling (e.g., convolutional layers) to reduce sequence length before self-attention, trading compute for a temporal-agnostic compression that can discard task-relevant information. We introduce \textbf{LAPSE} (\textbf{L}earned \textbf{A}ttention \textbf{P}ooling for \textbf{S}peech \textbf{E}ncoding), a learned downsampling module that performs \emph{content-adaptive segmentation} and \emph{segment-wise attention pooling} to produce a shorter sequence prior to the quadratic-cost Transformer/Conformer encoder.

Let an utterance be represented as a length-$T$ sequence of frame-level hidden states
\begin{equation}
\mathbf{H} = [\mathbf{h}_1, \ldots, \mathbf{h}_T], \qquad \mathbf{h}_t \in \mathbb{R}^{d}.
\end{equation}
LAPSE predicts boundaries between adjacent frames, partitioning the sequence into $M$ contiguous segments $\{\mathcal{S}_m\}_{m=1}^{M}$, and pools each segment into a single vector to obtain
\begin{equation}
\tilde{\mathbf{H}} = [\tilde{\mathbf{h}}_1, \ldots, \tilde{\mathbf{h}}_M], \qquad \tilde{\mathbf{h}}_m \in \mathbb{R}^{d'} , \qquad M \ll T.
\end{equation}
This shortened sequence is then processed by the downstream Conformer encoder and trained with a standard CTC objective. To directly control compression rate, we add an auxiliary count loss that encourages the expected number of segments $M$ to match a target count derived from an external phonemizer (phoneme- or syllable-level).


\subsection{Boundary Prediction}
\label{sec:lapse_boundary}
LAPSE assigns a probability of placing a boundary between each adjacent pair of frames. Each hidden state is projected into key and query vectors:
\begin{equation}
\mathbf{k}_t = \mathbf{W}_K \mathbf{h}_t, \qquad
\mathbf{q}_t = \mathbf{W}_Q \mathbf{h}_t,
\end{equation}
where $\mathbf{W}_K, \mathbf{W}_Q \in \mathbb{R}^{d_k \times d}$. We compute the cosine similarity between adjacent projections:
\begin{equation}
c_t = \cos(\mathbf{k}_t, \mathbf{q}_{t+1})
    = \frac{\mathbf{k}_t^\top \mathbf{q}_{t+1}}{\|\mathbf{k}_t\|_2\,\|\mathbf{q}_{t+1}\|_2},
\qquad t=1,\ldots,T-1.
\end{equation}
Intuitively, high similarity indicates continuity across frames and therefore \emph{lower} boundary likelihood. We convert similarity to a boundary probability by normalizing to the 0, 1 range:
\begin{equation}
p_t = \mathrm{clip}\!\left(\frac{1 - (c_t + b)}{2},\ 0,\ 1\right),
\label{eq:lapse_prob_clip}
\end{equation}
This mapping ensures $p_t \in [0,1]$ and directly ties boundary probabilities to local discontinuities: lower cosine similarity yields larger $p_t$.


\subsection{Differentiable Sampling}
\label{sec:lapse_sampling}
To obtain discrete boundaries $b_t \in \{0,1\}$ while enabling end-to-end training, we use the straight-through Gumbel-Sigmoid estimator \cite{jang2016categorical,maddison2016concrete}. During training, we sample $b_t$ from the Bernoulli distribution parameterized by $p_t$ using the reparameterization trick, allowing gradients to flow through the discrete samples. At inference, we simply threshold $p_t > 0.5$.

The boundary sequence $\{b_t\}_{t=1}^{T-1}$ partitions the frames into $M$ contiguous segments. Let the segments be indexed by $(\ell_m, r_m)$, where
\begin{equation}
\mathcal{S}_m = \{t \mid \ell_m \le t \le r_m\}, \qquad m=1,\ldots,M.
\end{equation}
By construction, $M = 1 + \sum_{t=1}^{T-1} b_t$.


\subsection{Attention Pooling}
\label{sec:lapse_pooling}
Given a segment $\mathcal{S}_m$, LAPSE compresses the frames in that segment into a single embedding using attention pooling. We first compute value projections:
\begin{equation}
\mathbf{v}_t = \mathbf{W}_V \mathbf{h}_t, \qquad \mathbf{W}_V \in \mathbb{R}^{d' \times d}.
\end{equation}
We then compute attention logits within the segment using a single learned pooling query vector used across all segments $\mathbf{u} \in \mathbb{R}^{d_a}$ and a projection $\mathbf{W}_A \in \mathbb{R}^{d_a \times d}$:
\begin{equation}
a_t = \langle \mathbf{u}, \mathbf{W}_A \mathbf{h}_t \rangle, \qquad t \in \mathcal{S}_m.
\end{equation}
The segment-local attention weights are
\begin{equation}
w_t = \frac{\exp(a_t)}{\sum_{j \in \mathcal{S}_m} \exp(a_j)}, \qquad t \in \mathcal{S}_m,
\end{equation}
and the pooled segment representation is
\begin{equation}
\tilde{\mathbf{h}}_m = \sum_{t \in \mathcal{S}_m} w_t \, \mathbf{v}_t.
\end{equation}
This mechanism allows the model to select the most information across frames within each predicted segment, rather than imposing a fixed pooling rule.



% Replace Sec. 3.5 (Rate control) with this version
% =========================

\subsection{Compression Rate Control}
\label{sec:lapse_rate_control}
Due to our boundary predictor optimizer, without explicit rate supervision, compression collapses to 0 or infinity. To target linguistically inspired boundaries with minimal supervision, we derive a target segment count $N^\star$ from the transcript using an external phonemizer, where $N^\star$ corresponds to either the phoneme count (phoneme-level) or syllable count (syllable-level). Since each boundary variable $b_t \in \{0,1\}$ indicates a split \emph{after} frame $t$, the number of segments is
\begin{equation}
M \;=\; 1 + \sum_{t=1}^{T-1} b_t,
\qquad
B \;=\; \sum_{t=1}^{T-1} b_t \;=\; M-1,
\end{equation}
where $B$ is the number of sampled boundaries.
We apply a binomial loss directly to the sampled boundary count $B$ produced by the (straight-through) Gumbel--Bernoulli sampler (Section~\ref{sec:lapse_sampling}). Concretely, let the target number of boundaries be
\begin{equation}
B^\star \;=\; N^\star - 1,
\end{equation}
and define a target boundary \emph{rate}
\begin{equation}
\rho^\star \;=\; \frac{B^\star}{T-1}.
\end{equation}
We then penalize deviations from the desired count using the negative log-likelihood of observing $B$ under a Binomial distribution with $n=T-1$ trials and success probability $\rho^\star$:
\begin{equation}
\mathcal{L}_{\mathrm{count}}
\;=\;
-\log \mathrm{Binomial}(B \mid T-1,\, \rho^\star).
\label{eq:lapse_binom_count}
\end{equation}
This loss enforces a global compression rate while allowing the model to freely choose \emph{where} boundaries occur based on local acoustics.

\paragraph{Targets.}
We consider two operating points:
(i) \textbf{phoneme-rate} targets, where $N^\star$ is the phoneme count (empirically targeting $\sim 10.4\times$ compression relative to the 10~ms input frame stride, i.e., $\sim 104$~ms per segment), and
(ii) \textbf{syllable-rate} targets, where $N^\star$ is the syllable count (targeting $\sim 28\times$ compression, i.e., $\sim 280$~ms per segment).

\subsection{Training objective}
\label{sec:lapse_objective}
LAPSE is trained end-to-end using (i) a CTC loss, (ii) an autoregressive decoder trained with standard cross-entropy, and (iii) the binomial count loss that controls compression (Section~\ref{sec:lapse_rate_control}). Let $\mathcal{L}_{\mathrm{CTC}}$ denote the CTC objective computed from the encoder outputs, and let $\mathcal{L}_{\mathrm{CE}}$ denote the token-level cross-entropy loss of the ASR decoder over the target transcript tokens. We combine the recognition losses with fixed weights:
\begin{equation}
\mathcal{L}_{\mathrm{rec}} \;=\; 0.3\,\mathcal{L}_{\mathrm{CTC}} \;+\; 0.7\,\mathcal{L}_{\mathrm{CE}}.
\label{eq:lapse_recognition_loss}
\end{equation}
We then add the compression rate-control term:
\begin{equation}
\mathcal{L} \;=\; \mathcal{L}_{\mathrm{rec}} \;+\; \lambda\,\mathcal{L}_{\mathrm{count}},
\label{eq:lapse_full_objective}
\end{equation}
where $\mathcal{L}_{\mathrm{count}}$ is the binomial loss on sampled boundary counts defined in Eq.~\eqref{eq:lapse_binom_count}, and $\lambda$ controls the strength of rate control relative to recognition.


\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Datasets}
We evaluate LAPSE primarily on LibriSpeech 960h in the fully supervised ASR setting. We report results on the standard test-clean and test-other evaluation sets, and use dev-clean/dev-other for validation and model selection. Audio is resampled to 16 kHz and represented as 80-channel log-mel filterbank features computed with 25ms windows and 10ms stride. We apply standard speech augmentations during training, including Speed Perturbation (95\%, 100\%, 105\%) and SpecAugment (time masking, frequency masking, and time warping).

To test whether learned segmentation generalizes beyond ASR, we additionally run a small-scale speech-to-text translation experiment on [TBD dataset: e.g., MuST-C / CoVoST2 / IWSLT], reusing the same LAPSE module and training recipe except for the task-specific output vocabulary and objective.

\subsection{Model architecture}
Our base model is a Conformer encoder-decoder trained from scratch with approximately 120M parameters. LAPSE is inserted after the initial feature extractor and convolutional front-end, and before the Conformer blocks, replacing (or augmenting) fixed subsampling that is typically used to reduce frame rate prior to self-attention.

Unless noted otherwise, all models share the same downstream Conformer architecture and differ only in their downsampling strategy. For LAPSE, we target two operating regimes:
\begin{itemize}
    \item \textbf{Phoneme-rate} encoding, with an average compression of approximately $10.4\times$ relative to the pre-LAPSE frame sequence length.
    \item \textbf{Syllable-rate} encoding, with an average compression of approximately $28\times$.
\end{itemize}

\subsection{Optimization and training details}
All models are trained for 70 epochs using the Adam optimizer ($\beta_1=0.9, \beta_2=0.98, \epsilon=10^{-9}$) with a Noam learning rate schedule (15k warmup steps). We use a global batch size of 256 and apply gradient clipping of 5.0. We find that tuning the rate-control weight $\lambda$ has negligible impact on the final compression rate, as the model consistently converges to the target count regardless of even huge variations in $\lambda$ (e.g. from 0.1 to 10). Decoding uses a beam size of 66 with a Transformer LM (weight 0.60) and CTC scoring (weight 0.40).

We implement all models in SpeechBrain and train on NVIDIA GPUs. Unless otherwise stated, we use identical training and decoding hyperparameters across baselines to isolate the impact of downsampling.

\subsection{Baselines}
We compare LAPSE against the following downsampling approaches:
\begin{enumerate}
    \item \textbf{Convolutional subsampling} (fixed stride). Standard strided convolutional front-end that reduces the frame rate by a fixed factor (e.g., $2\times, 4\times$).
    \item \textbf{Fixed-stride pooling/downsampling}. Uses the \emph{same attention pooling module} as LAPSE, but replaces learned boundary prediction with \emph{deterministic, evenly spaced boundaries} at a fixed stride to match LAPSE's average compression. This isolates the effect of \emph{learned boundaries} from the pooling mechanism itself.
    \item \textbf{Token merging / similarity-based compression}. Learned merging approaches that compress by merging similar representations (applied at the same insertion point as LAPSE).
    \item \textbf{Alignment-supervised boundaries (MFA)}. A boundary prediction variant trained with supervision from forced alignments produced by Montreal Forced Aligner, used to assess whether explicit boundary supervision improves or harms ASR relative to learned boundaries.
\end{enumerate}

Where applicable, we report two comparison modes:
\begin{itemize}
    \item \textbf{Matched compression}: compare methods at similar average output sequence length $M$.
    \item \textbf{Matched accuracy}: compare methods at similar WER and report achieved compression.
\end{itemize}

\subsection{Evaluation metrics}
\textbf{ASR quality.} We report word error rate (WER) on LibriSpeech test-clean and test-other.

\textbf{Compression.} We report (i) the average compression factor $T/M$, where $T$ is the pre-downsampling length at the LAPSE input and $M$ is the number of segments emitted by LAPSE, and (ii) the tokens-per-second rate at the LAPSE output. For completeness, we also report the distribution (mean/median/percentiles) of $M$ across utterances, since quadratic attention cost is dominated by longer sequences.

\textbf{Boundary alignment.} To assess whether LAPSE learns linguistically meaningful boundaries, we compare predicted boundaries against forced-alignment boundaries from MFA. We report boundary F1 (with 20ms tolerance) and provide qualitative visualizations overlaying predicted boundaries on spectrograms.

\textbf{Efficiency.} When feasible, we measure encoder wall-clock time and/or compute proxies such as self-attention FLOPs proportional to $M^2$, to quantify the computational benefit of learned downsampling.

\subsection{Implementation details and reproducibility}
We release implementation details necessary to reproduce LAPSE, including boundary predictor dimensions, Gumbel temperature schedule, and the pooling mechanism. For all experiments we fix random seeds and report results from single runs.

\subsubsection{Sections and Subsections}

Section headings should be numbered, flush left, and set in 11~pt bold
type with the content words capitalized. Leave 0.25~inches of space
before the heading and 0.15~inches after the heading.

Similarly, subsection headings should be numbered, flush left, and set
in 10~pt bold type with the content words capitalized. Leave
0.2~inches of space before the heading and 0.13~inches afterward.

Finally, subsubsection headings should be numbered, flush left, and
set in 10~pt small caps with the content words capitalized. Leave
0.18~inches of space before the heading and 0.1~inches after the
heading.

Please use no more than three levels of headings.

\subsubsection{Paragraphs and Footnotes}

Within each section or subsection, you should further partition the
paper into paragraphs. Do not indent the first line of a given
paragraph, but insert a blank line between succeeding ones.

You can use footnotes\footnote{Footnotes
should be complete sentences.} to provide readers with additional
information about a topic without interrupting the flow of the paper.
Indicate footnotes with a number in the text where the point is most
relevant. Place the footnote in 9~point type at the bottom of the
column in which it appears. Precede the first footnote in a column
with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
appear in each column, in the same order as they appear in the text,
but spread them across columns and pages if possible.}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
\caption{Historical locations and number of accepted papers for International
Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
produced, the number of accepted papers for ICML 2008 was unknown and instead
estimated.}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Figures}

You may want to include figures in the paper to illustrate
your approach and results. Such artwork should be centered,
legible, and separated from the text. Lines should be dark and at
least 0.5~points thick for purposes of reproduction, and text should
not appear on a gray background.

Label all distinct components of each figure. If the figure takes the
form of a graph, then give a name for each axis and include a legend
that briefly describes each curve. Do not include a title inside the
figure; instead, the caption should serve this function.

Number figures sequentially, placing the figure number and caption
\emph{after} the graphics, with at least 0.1~inches of space before
the caption and 0.1~inches after it, as in
\cref{icml-historical}. The figure caption should be set in
9~point type and centered unless it runs two or more lines, in which
case it should be flush left. You may float figures to the top or
bottom of a column, and you may set wide figures across both columns
(use the environment \texttt{figure*} in \LaTeX). Always place
two-column figures at the top or bottom of the page.

\subsection{Algorithms}

If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
environments to format pseudocode. These require
the corresponding stylefiles, algorithm.sty and
algorithmic.sty, which are supplied with this package.
\cref{alg:example} shows an example.

\begin{algorithm}[tb]
   \caption{Bubble Sort}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $x_i$, size $m$
   \REPEAT
   \STATE Initialize $noChange = true$.
   \FOR{$i=1$ {\bfseries to} $m-1$}
   \IF{$x_i > x_{i+1}$}
   \STATE Swap $x_i$ and $x_{i+1}$
   \STATE $noChange = false$
   \ENDIF
   \ENDFOR
   \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\subsection{Tables}

You may also want to include tables that summarize material. Like
figures, these should be centered, legible, and numbered consecutively.
However, place the title \emph{above} the table with at least
0.1~inches of space before the title and the same after it, as in
\cref{sample-table}. The table title should be set in 9~point
type and centered unless it runs two or more lines, in which case it
should be flush left.

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

\begin{table}[t]
\caption{Classification accuracies for naive Bayes and flexible
Bayes on various data sets.}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Data set & Naive & Flexible & Better? \\
\midrule
Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Tables contain textual material, whereas figures contain graphical material.
Specify the contents of each row and column in the table's topmost
row. Again, you may float tables to a column's top or bottom, and set
wide tables across both columns. Place two-column tables at the
top or bottom of the page.

\subsection{Theorems and such}
The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
\begin{definition}
\label{def:inj}
A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
\end{definition}
Using \cref{def:inj} we immediate get the following result:
\begin{proposition}
If $f$ is injective mapping a set $X$ to another set $Y$, 
the cardinality of $Y$ is at least as large as that of $X$
\end{proposition}
\begin{proof} 
Left as an exercise to the reader. 
\end{proof}
\cref{lem:usefullemma} stated next will prove to be useful.
\begin{lemma}
\label{lem:usefullemma}
For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
\end{lemma}
\begin{theorem}
\label{thm:bigtheorem}
If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
\end{theorem}
An easy corollary of \cref{thm:bigtheorem} is the following:
\begin{corollary}
If $f:X\to Y$ is bijective, 
the cardinality of $X$ is at least as large as that of $Y$.
\end{corollary}
\begin{assumption}
The set $X$ is finite.
\label{ass:xfinite}
\end{assumption}
\begin{remark}
According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
\end{remark}
%restatable

\subsection{Citations and References}

Please use APA reference format regardless of your formatter
or word processor. If you rely on the \LaTeX\/ bibliographic
facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
included in the style-file package to obtain this format.

Citations within the text should include the authors' last names and
year. If the authors' names are included in the sentence, place only
the year in parentheses, for example when referencing Arthur Samuel's
pioneering work \yrcite{Samuel59}. Otherwise place the entire
reference in parentheses with the authors and year separated by a
comma \cite{Samuel59}. List multiple references separated by
semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
construct only for citations with three or more authors or after
listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

Authors should cite their own work in the third person
in the initial version of their paper submitted for blind review.
Please refer to \cref{author info} for detailed instructions on how to
cite your own papers.

Use an unnumbered first-level section heading for the references, and use a
hanging indent style, with the first line of the reference flush against the
left margin and subsequent lines indented by 10 points. The references at the
end of this document give examples for journal articles \cite{Samuel59},
conference publications \cite{langley00}, book chapters \cite{Newell81}, books
\cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
\cite{mitchell80}, and dissertations \cite{kearns89}.

Alphabetize references by the surnames of the first authors, with
single author entries preceding multiple author entries. Order
references for the same authors by year of publication, with the
earliest first. Make sure that each reference includes all relevant
information (e.g., page numbers).

Please put some effort into making references complete, presentable, and
consistent, e.g. use the actual current name of authors.
If using bibtex, please protect capital letters of names and
abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
in your .bib file.

\section*{Accessibility}
Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

\section*{Software and Data}

If a paper is accepted, we strongly encourage the publication of software and data with the
camera-ready version of the paper whenever appropriate. This can be
done by including a URL in the camera-ready copy. However, \textbf{do not}
include URLs that reveal your institution or identity in your
submission for review. Instead, provide an anonymous URL or upload
the material as ``Supplementary Material'' into the OpenReview reviewing
system. Note that reviewers are not required to look at this material
when writing their review.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

\textbf{Do not} include acknowledgements in the initial version of
the paper submitted for blind review.

If a paper is accepted, the final camera-ready version can (and
usually should) include acknowledgements.  Such acknowledgements
should be placed at the end of the section, in an unnumbered section
that does not count towards the paper page limit. Typically, this will 
include thanks to reviewers who gave useful comments, to colleagues 
who contributed to the ideas, and to funding agencies and corporate 
sponsors that provided financial support.

\section*{Impact Statement}

Authors are \textbf{required} to include a statement of the potential 
broader impact of their work, including its ethical aspects and future 
societal consequences. This statement should be in an unnumbered 
section at the end of the paper (co-located with Acknowledgements -- 
the two may appear in either order, but both must be before References), 
and does not count toward the paper page limit. In many cases, where 
the ethical impacts and expected societal implications are those that 
are well established when advancing the field of Machine Learning, 
substantial discussion is not required, and a simple statement such 
as the following will suffice:

``This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.''

The above statement can be used verbatim in such cases, but we 
encourage authors to think about whether there is content which does 
warrant further discussion, as this statement will be apparent if the 
paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$ pages long.
For the final version, one more page can be added.
If you want, you can use an appendix like this one.  

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
